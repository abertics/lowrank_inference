"""
August 2021.

Trying out different methods for fitting low-rank networks to data generated by a full-rank network performing Mante task.
"""
from low_rank_rnns.modules import *
from low_rank_rnns import mante, stats

size = 1024
noise_std = 5e-2
alpha = .2
n_samples = 10

x_train, y_train, mask_train, x_val, y_val, mask_val = mante.generate_mante_data(1000)
net = FullRankRNN(4, size, 1, noise_std, alpha)

net.load_state_dict(torch.load(f'../models/mante_fr_1024_0.1_1e-3.pt'))
loss, acc = mante.test_mante(net, x_val, y_val, mask_val)
print(f'loss={loss:.3f}, acc={acc:.3f}')

# Generate target trajectories
output, traj = net.forward(x_train, return_dynamics=True)
target = torch.tanh(traj[:, 1:].detach())
T = x_train.shape[1]
out1, traj1 = net.forward(x_val, return_dynamics=True)
traj1 = net.non_linearity(traj1)

# Loop over subsample sizes
ps = [.15, .1, .05, .02, .01]
r2s_lr = np.zeros((len(ps), n_samples))
accs_lr = np.zeros((len(ps), n_samples))
losses_lr = np.zeros((len(ps), n_samples))
r2s_fr = np.zeros((len(ps), n_samples))
accs_fr = np.zeros((len(ps), n_samples))
losses_fr = np.zeros((len(ps), n_samples))

for k, p in enumerate(ps):
    size_sel = int(floor(size * p))
    for i in range(n_samples):
        # Training low-rank RNN
        neurons_sel = np.random.choice(np.arange(size), size=size_sel, replace=False)
        target_sel = target[:, :, neurons_sel]
        net2 = LowRankRNN(4, size_sel, size_sel, noise_std, alpha, rank=1,
                          wo_init=size_sel * torch.from_numpy(np.eye(size_sel)), train_wi=True, train_so=False)
        train(net2, x_train, target_sel, torch.ones((x_train.shape[0], T, 1)), 200, lr=1e-2, clip_gradient=1, keep_best=True, cuda=True)
        net2.to('cpu')
        torch.save(net2.state_dict(), f'../models/mante_many/mante_subs_lr_{k}_{i}.pt')
        _, traj2 = net2.forward(x_val, return_dynamics=True)
        traj2 = net2.non_linearity(traj2)
        y2 = traj2.detach().numpy().ravel()
        y1 = traj1[:, :, neurons_sel].detach().numpy().ravel()
        r2 = stats.r2_score(y1, y2)
        print(r2)
        r2s_lr[k, i] = r2
        net2.wo = nn.Parameter(net.wo_full[neurons_sel].clone())
        net2.output_size = 1
        net2.so = nn.Parameter(torch.tensor([1. * size_sel]))
        loss, acc = mante.test_mante(net2, x_val, y_val, mask_val)
        losses_lr[k, i] = loss
        accs_lr[k, i] = acc

        # All the same for the full-rank RNN
        net2 = FullRankRNN(4, size_sel, size_sel, noise_std, alpha,
                          wo_init=torch.from_numpy(np.eye(size_sel)), train_wi=True, train_so=False, train_wo=False)
        train(net2, x_train, target_sel, torch.ones((x_train.shape[0], T, 1)), 200, lr=1e-4, clip_gradient=1, keep_best=True, cuda=True)
        net2.to('cpu')
        torch.save(net2.state_dict(), f'../models/mante_subs_fr_{k}_{i}.pt')
        _, traj2 = net2.forward(x_val, return_dynamics=True)
        traj2 = net2.non_linearity(traj2)
        y2 = traj2.detach().numpy().ravel()
        r2 = stats.r2_score(y1, y2)
        print(r2)
        r2s_fr[k, i] = r2
        net2.wo = nn.Parameter(net.wo_full[neurons_sel].clone())
        net2.output_size = 1
        net2.so = nn.Parameter(torch.tensor([1. * size_sel]))
        loss, acc = mante.test_mante(net2, x_val, y_val, mask_val)
        losses_fr[k, i] = loss
        accs_fr[k, i] = acc


np.savez(f'../data/mante_subsampling.npz', r2s_lr, accs_lr, losses_lr, r2s_fr, accs_fr, losses_fr)
